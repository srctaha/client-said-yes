<!doctype html>
<html lang="en-us" dir="ltr" class="kind-page">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Sercan Ahi">
    <meta name="description" content="Enterprises rarely operate a single Databricks Workspace. Instead, they often maintain separate development, testing, production, and region-specific …">

    <title>Cross-Workspace Delta Lake Table Lineage in PySpark: Tracking Data Movement Across Multiple Databricks Workspaces</title>
    <link rel="canonical" href="http://localhost:1313/posts/databricks-cross-workspace-lineage/">



<meta property="og:title" content="Cross-Workspace Delta Lake Table Lineage in PySpark: Tracking Data Movement Across Multiple Databricks Workspaces">
<meta property="og:description" content="Enterprises rarely operate a single Databricks Workspace. Instead, they often maintain separate development, testing, production, and region-specific …">
<meta property="og:image" content="http://localhost:1313/og/og-image-ab519ac324253a06e3cba12ae8b7a614.svg">
<meta property="og:image:type" content="image/svg+xml">
<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="630">
<meta property="og:type" content="website">
<meta property="og:url" content="http://localhost:1313/posts/databricks-cross-workspace-lineage/">
<meta property="og:locale" content="en-us">


<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:title" content="Cross-Workspace Delta Lake Table Lineage in PySpark: Tracking Data Movement Across Multiple Databricks Workspaces">
<meta name="twitter:description" content="Enterprises rarely operate a single Databricks Workspace. Instead, they often maintain separate development, testing, production, and region-specific …">
<meta name="twitter:image" content="http://localhost:1313/og/og-image-ab519ac324253a06e3cba12ae8b7a614.svg">
<meta name="twitter:creator" content="@YourXHandle">

    <link rel="preload" href="/fonts/Newsreader.woff2" as="font" type="font/woff2" crossorigin>
    <link rel="preload" href="/fonts/Newsreader-italic.woff2" as="font" type="font/woff2" crossorigin>
    <link rel="stylesheet" href="/css/default.min.7cf2b9c1994c9982455e0514bfcef87693f541b72b8359792c45c21d67968a34.css" integrity="sha256-fPK5wZlMmYJFXgUUv874dpP1Qbcrg1l5LEXCHWeWijQ=">

    <link rel="icon" type="image/x-icon" href="/favicon.ico">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <script src="/js/darkmode.min.9211af6d380489c7cf64d4b18a1e49e75be2bebf92031d2165eb6c20bc59c6a2.js" integrity="sha256-khGvbTgEicfPZNSxih5J51vivr&#43;SAx0hZetsILxZxqI=" defer></script>
    <script src="/js/footnotes.min.e049c26d7ddda424be30c222001b5dbd0890e1bd0659fa171e586e92158e1286.js" integrity="sha256-4EnCbX3dpCS&#43;MMIiABtdvQiQ4b0GWfoXHlhukhWOEoY=" defer></script>
</head>


<body>
<header>
    <div>
        <h1>
            <a href="http://localhost:1313/">Personal log</a>
        </h1>
        <div class="header-controls">
            <div class="toggle-switch">
                <input type="checkbox" id="darkModeToggle" class="toggle-input" aria-label="Toggle dark mode">
                <label for="darkModeToggle" class="toggle-label">
                    <span class="toggle-slider"></span>
                </label>
            </div>
        </div>
    </div>
</header>


    <main class="content-wrapper">
        <article class="content-container">


    <h1>Cross-Workspace Delta Lake Table Lineage in PySpark: Tracking Data Movement Across Multiple Databricks Workspaces</h1>
    <h4></h4>
    <p class="author-date"><time datetime="2025-08-16T00:00:00Z">August 16, 2025</time></p>

<p>Enterprises rarely operate a single Databricks Workspace. Instead, they often maintain separate development, testing, production, and region-specific workspaces to meet compliance, security, or scalability needs.</p>
<p>While Unity Catalog captures lineage inside a workspace, tracking lineage <strong>across multiple workspaces</strong> remains under-documented. Yet, in many organizations, data pipelines span workspaces: a dev pipeline writes to a bronze table, production pipelines consume from it, and regional clusters replicate tables for local analytics.</p>
<p>This article shows how PySpark and Delta Lake transaction logs can be used to reconstruct lineage across workspace boundaries.</p>
<h2 id="why-does-cross-workspace-lineage-matter">Why does cross-workspace lineage matter?<a hidden class="anchor" aria-hidden="true" href="#why-does-cross-workspace-lineage-matter">#</a></h2>
<ul>
<li><strong>Audit &amp; Compliance</strong>: Regulators often require visibility into how data flows across regions (e.g., proving that EU customer data never leaves the EU).</li>
<li><strong>Operational Reliability</strong>: Understanding dependencies helps avoid breakage when upstream data changes.</li>
<li><strong>Business Continuity</strong>: When a pipeline fails, tracing downstream dependencies reduces data downtime.</li>
</ul>
<p>By analyzing the <strong>Delta</strong> <code>_delta_log</code>, we can reconstruct event-level lineage across environments.</p>
<h2 id="what-is-_delta_log">What is <code>_delta_log</code>?<a hidden class="anchor" aria-hidden="true" href="#what-is-_delta_log">#</a></h2>
<ul>
<li>A <strong>hidden folder</strong> inside every Delta Table&rsquo;s storage directory.</li>
<li>It contains <strong>metadata about every transaction</strong> that has ever happened to that table.</li>
<li>Think of it as the <strong>transaction log</strong> or the <strong>brain</strong> of the Delta Table.</li>
</ul>
<h2 id="okay-but-what-is-delta-table-and-how-is-it-related-to-databricks-workspaces">Okay, but what is Delta Table and how is it related to Databricks Workspaces?<a hidden class="anchor" aria-hidden="true" href="#okay-but-what-is-delta-table-and-how-is-it-related-to-databricks-workspaces">#</a></h2>
<ul>
<li>Delta Table is the fundamental storage unit of Delta Lake, which is an open-source storage layer that powers reliable data management in Databricks Workspaces and elsewhere.</li>
<li>In simple terms, it is a <strong>Parquet-based table enhanced with a transaction log</strong> (<code>_delta_log</code>) that brings database-like reliability to your cloud data lake.</li>
</ul>
<h2 id="what-do-we-have-inside-delta-transaction-logs">What do we have inside delta transaction logs?<a hidden class="anchor" aria-hidden="true" href="#what-do-we-have-inside-delta-transaction-logs">#</a></h2>
<p>Every Delta table contains a hidden directory called <code>_delta_log</code>. This folder stores:</p>
<ul>
<li>JSON commit files (<code>00000000000000000010.json, ...</code>) — one per transaction.</li>
<li>Parquet checkpoints (<code>00000000000000001000.checkpoint.parquet</code>) — snapshots of the full table state for fast recovery.</li>
</ul>
<p>Commit JSON files are particularly valuable for lineage. They capture:</p>
<ul>
<li><strong>Who</strong> wrote the data (user name)?</li>
<li><strong>When</strong> it was written (timestamp)?</li>
<li>How it was written (operation, e.g., <code>WRITE</code>, <code>MERGE</code>, <code>DELETE</code>)?</li>
<li>What files were added or removed?</li>
</ul>
<h2 id="how-can-we-locate-_delta_log-in-our-workspaces">How can we locate <code>_delta_log</code> in our workspaces?<a hidden class="anchor" aria-hidden="true" href="#how-can-we-locate-_delta_log-in-our-workspaces">#</a></h2>
<p>If you want to work with a specific table (for example, <code>table_name = &quot;main.analytics.my_table&quot;</code>) in a Unity Catalog–enabled workspace, you can choose from at least two approaches.</p>
<h3 id="1-unity-catalog-recommended">1. Unity Catalog (recommended)<a hidden class="anchor" aria-hidden="true" href="#1-unity-catalog-recommended">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>loc <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>    spark
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>sql(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;DESCRIBE DETAIL </span><span style="color:#e6db74">{</span>table_name<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>select(<span style="color:#e6db74">&#34;location&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>first()[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>log_path <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>loc<span style="color:#f92672">.</span>rstrip(<span style="color:#e6db74">&#39;/&#39;</span>)<span style="color:#e6db74">}</span><span style="color:#e6db74">/_delta_log/*.json&#34;</span>
</span></span></code></pre></div><h3 id="2-deltatable-api">2. DeltaTable API<a hidden class="anchor" aria-hidden="true" href="#2-deltatable-api">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> delta.tables <span style="color:#f92672">import</span> DeltaTable
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dt <span style="color:#f92672">=</span> DeltaTable<span style="color:#f92672">.</span>forName(spark, table_name)
</span></span><span style="display:flex;"><span>loc <span style="color:#f92672">=</span> dt<span style="color:#f92672">.</span>detail()<span style="color:#f92672">.</span>select(<span style="color:#e6db74">&#34;location&#34;</span>)<span style="color:#f92672">.</span>first()[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>log_path <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>loc<span style="color:#f92672">.</span>rstrip(<span style="color:#e6db74">&#39;/&#39;</span>)<span style="color:#e6db74">}</span><span style="color:#e6db74">/_delta_log/*.json&#34;</span>
</span></span></code></pre></div><h2 id="how-can-we-parse-the-commits-to-delta-tables-with-pyspark">How can we parse the commits to Delta Tables with PySpark?<a hidden class="anchor" aria-hidden="true" href="#how-can-we-parse-the-commits-to-delta-tables-with-pyspark">#</a></h2>
<p>Once you have <code>log_path</code>, you can parse the commit logs as follows.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql.functions <span style="color:#f92672">import</span> col, input_file_name, regexp_extract, to_timestamp
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>commits_df <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>    spark
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>read
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>json(log_path)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Capture file names</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>withColumn(<span style="color:#e6db74">&#34;file&#34;</span>, input_file_name())
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Extract numeric version from the filename (e.g., 00000000000000000123.json -&gt; 123)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>withColumn(
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;version&#34;</span>,
</span></span><span style="display:flex;"><span>        regexp_extract(col(<span style="color:#e6db74">&#34;file&#34;</span>), <span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;/_delta_log/0+([0-9]+)\.json$&#34;</span>, <span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>cast(<span style="color:#e6db74">&#34;long&#34;</span>)
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>dropDuplicates()
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>commits_df<span style="color:#f92672">.</span>display()
</span></span></code></pre></div><h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<ul>
<li>Tracking lineage across Databricks Workspaces is a gap in most documentation, yet critical for governance and reliability.</li>
<li>With PySpark and Delta transaction logs, teams can build a custom, audit-ready lineage system that spans environments.</li>
<li>This not only improves compliance and reduces risk but also builds trust in enterprise analytics.</li>
</ul>
<p>⚡️ Call to Action: If you are running multi-workspace Databricks environments, start small and parse a single <code>_delta_log</code> with PySpark, link it to Unity Catalog, and see how far you can trace your data&rsquo;s journey.</p>



        </article>
    </main>

<footer>
    <div class="footer-content">
        <p>Copyright © 2025 Sercan Ahi. All rights reserved.</p>
    </div>
</footer>

</body>

</html>
